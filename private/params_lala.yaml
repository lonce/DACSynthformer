experiment: "2025.03.07b_lala_test_256_float_FiLM"

#data_dir: "/home/lonce/scratchdata/Lala_data/lala_dac"
#data_frames: "/home/lonce/scratchdata/Lala_data/lala-train.xlsx"
data_dir: "/home/lonce/scratchdata/Lala_data/lala_dac"
data_frames: "/home/lonce/scratchdata/Lala_data/lala_param_train.xlsx"
validator_data_dir: null
validator_data_frames: null


TransformerClass: "RopeCondDACTransformer" 
vocab_size: 1024
num_tokens: 4

#cond_params: 1 #1 (not counting the classes)
model_size: 256 # must be divisible by num_heads

Ti: 43 #86 # 172 #86 mask size for training and training, mask and windowsize for inference 
Tt: 430 # must match the length of the sequences in the batch
batch_size: 4  #**


num_layers: 4 #**
num_heads: 8 #8 # 8 # embed_size must be divisible by num_heads
forward_expansion: 4 #4 #4
dropout_rate: 0.2
learning_rate: 0.0005
use_adaLN: False #else FiLM

num_epochs: 10000 ### 800 

ErrorLogRate: 100 #2 ### 10
checkpoint_interval: 1000 ###50 # 25

