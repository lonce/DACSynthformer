{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9bb5a36d-0fea-4c17-b799-f7bae8b0db61",
   "metadata": {},
   "source": [
    "### chaptGPT specs   \n",
    "\n",
    "A decoder-only transformer in pytorch to predict 'next output' at each time step. \n",
    "\n",
    "Each time step t is represented by a vector of n=4 tokens from the Descript DAC encoder. \n",
    "The length of the sequence (context window) is Ti=86 for inference, and Tt=8*Ti for training. That is, the context window for training is 8 times the length of the context window for inference. \n",
    "The attention is \"causal\", looking only back in time, and the maximum look-back time for the attention blocks is Ti (even when the sequence is longer during training). That is, the masking matrix is *banded* - triangular to be causal, and limited in lookback which results in a diagonal band). This prevents much of the training on shortened context that happens when tokens are near the beginning of traning examples. \n",
    "\n",
    "The size of the vocabulary (the number of descrete values in each codebook) for each of the n tokens is V=1024. \n",
    "\n",
    "The dataloader will as is usual, supply batches in triplets  (input, target, conditioning info) where the size of each input and output is Tt*n (the sequence length times the number of tokens at each time step). The tokens are indexes for the vocabulary in the range of (0, V-1). The targets are shifted relative to the input sequence by 1 as is typical for networks the learn to predict the output for the next time step. \n",
    "\n",
    "The first layer in the architecture will be a learnable \"multiembedding\" layer that embeds each of the 4 tokens at each time step as an m-dimensional vector. The n m-dimensional vectors are concatenated to provide the n*m dimensional input embeddings for the transformer blocks at each time step. \n",
    "\n",
    "A positional code is is added to the K and Q matricies in each Transformer block using Rotary Position Embedding (RoPE).\n",
    "\n",
    "We use a stack of b transformer blocks that are standard (using layer norms, a relu for activation, and a forward expansion factor of 4 form the linear layer). Each transformer block consumes and produces a context window length sequence of m*n dimensional vectors. \n",
    "\n",
    "After the last transformer block, there is a linear layer that maps the m*n dimensional vectors to the output size which is V*n (the vocabulary size time the number of tokens stacked at each time step). These are the logits that will be fed to the softmax functions (one for each of the n to kens) that provide the probability distribtion across the vocabulary set. We use the criterion nn.CrossEntropyLoss() for computing the loss using the targets provided by the dataloader, and Adam for the optimizer.\n",
    "\n",
    "Again, at inference time, the fixed-length context window is shorter than the training sequence window length, and equal to the maximum look-back time of the attention blocks. The inference process takes the output produced at each time step (a stack of n tokens), and shift them in to a sliding window that is used for input for the next time step. The length of the sequences generated during inference is arbitrary and should be settable with a parameter. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8db8df-da27-4eb1-91c5-1d4945ff4161",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ce614c-47d9-4e16-bee8-3fbcc556b08a",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad58d43c-b453-496f-8a43-f0a1722b8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramfile = \"private/params_lala-mini.yaml\" # 'params.yaml' #\n",
    "paramfile = \"params_nsynth64.76.yaml # 'params.yaml' #\n",
    "DEVICE='cuda' ##''cuda'\n",
    "start_epoch=0 # to start from a previous training checkpoint, otherwise must be 0\n",
    "verboselevel=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9760e14-8c2f-4a14-be94-f1ef300296ce",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d67b59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# and for creating a custom dataset and loader:\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import yaml\n",
    "import shutil\n",
    "\n",
    "from utils.utils import generate_mask, save_model, load_model, writeDACFile, interpolate_vectors\n",
    "from DACTransformer.RopeCondDACTransformer import RopeCondDACTransformer\n",
    "\n",
    "from dataloader.dataset import CustomDACDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76dcc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b85ef",
   "metadata": {},
   "source": [
    "### <font color='blue'> Derived parameters </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "638ee684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has for conditioning 1 classes and 1 parameters.\n",
      "class names are ['solfege'] and params are ['note']\n",
      "embed_size is 256\n",
      "using TransformerClass = RopeCondDACTransformer\n",
      "basefname = out.e256.l4.h8\n",
      "outdir = runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/params.yaml'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training data dir\n",
    "\n",
    "# Load YAML file\n",
    "with open(paramfile, 'r') as file:\n",
    "    params = yaml.safe_load(file)\n",
    "\n",
    "data_dir = params['data_dir']\n",
    "data_frames =  params['data_frames']\n",
    "validator_data_dir = params['validator_data_dir']\n",
    "validator_data_frames = params['validator_data_frames']\n",
    "\n",
    "Ti = params['Ti'] # mask size\n",
    "Tt = params['Tt'] # context length (sequence steps for training = timesteps in trainingset -1)\n",
    "\n",
    "# Create an instance of the dataset\n",
    "dataset = CustomDACDataset(data_dir=data_dir, metadata_excel=data_frames, Tt=Tt, transforms=None)\n",
    "\n",
    "# ---------     for the transformer  --------------#\n",
    "vocab_size = params['vocab_size']\n",
    "num_tokens = params['num_tokens']\n",
    "\n",
    "cond_classes = dataset.get_num_classes() # 0\n",
    "cond_params = dataset.get_num_params()\n",
    "cond_size = cond_classes + cond_params # num_classes + num params - not a FREE parameter!\n",
    "print(f'Dataset has for conditioning {cond_classes} classes and {cond_params} parameters.')\n",
    "print(f'class names are {dataset.get_class_names()} and params are {dataset.get_param_names()}')\n",
    "\n",
    "#embed_size = params['tblock_input_size'] -cond_size # 240 #32  # embed_size +cond_size must be divisible by num_heads and by num tokens\n",
    "embed_size = params['model_size']  # embed_size  must be divisible by num_heads and by num tokens\n",
    "print(f'embed_size is {embed_size}')\n",
    "\n",
    "batch_size = params['batch_size']\n",
    "\n",
    "sequence_length = Tt  # For training\n",
    "\n",
    "num_layers = params['num_layers']\n",
    "num_heads = params['num_heads']\n",
    "forward_expansion = params['forward_expansion']\n",
    "dropout_rate = params['dropout_rate']\n",
    "learning_rate = params['learning_rate']\n",
    "use_adaLN = params['use_adaLN']\n",
    "num_epochs=params['num_epochs']\n",
    "\n",
    "experiment_name=params['experiment'] \n",
    "outdir = 'runs' + '/' + experiment_name\n",
    "basefname= 'out' + '.e' + str(embed_size) + '.l' + str(num_layers) + '.h' + str(num_heads) \n",
    "\n",
    "ErrorLogRate = params['ErrorLogRate'] #10\n",
    "checkpoint_interval = params['checkpoint_interval']\n",
    "\n",
    "\n",
    "\n",
    "TransformerClass =  globals().get(params['TransformerClass'])  \n",
    "\n",
    "print(f\"using TransformerClass = {params['TransformerClass']}\") \n",
    "print(f'basefname = {basefname}')\n",
    "print(f'outdir = {outdir}')\n",
    "\n",
    "###########################################################################\n",
    "# Ensure the destination directory exists\n",
    "#destination_dir = os.path.dirname(outdir + '/' + paramfile)\n",
    "#if not os.path.exists(destination_dir):\n",
    "#    os.makedirs(destination_dir)\n",
    "    \n",
    "if not os.path.exists(outdir):\n",
    "    os.makedirs(outdir)\n",
    "shutil.copy(paramfile, outdir + '/params.yaml')  # copy whatever paramfile was used to outdir and name it params.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebf3928",
   "metadata": {},
   "source": [
    "### <font color='blue'> Set up cuda. \n",
    "Without it, training runs about 10 times slower  \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ff0adcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memeory on cuda 0 is  25.37816064\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if DEVICE == 'cuda' :\n",
    "    torch.cuda.device_count()\n",
    "    torch.cuda.get_device_properties(0).total_memory/1e9\n",
    "\n",
    "    device = torch.device(DEVICE) # if the docker was started with --gpus all, then can choose here with cuda:0 (or cpu)\n",
    "    torch.cuda.device_count()\n",
    "    print(f'memeory on cuda 0 is  {torch.cuda.get_device_properties(0).total_memory/1e9}')\n",
    "else :\n",
    "    device=DEVICE\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee7020c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Load data \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c684557b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1\n",
      "Inputs shape: torch.Size([4, 430, 4])\n",
      "Targets shape: torch.Size([4, 430, 4])\n",
      "cvect shape: torch.Size([4, 2])\n",
      "cevect is tensor([[1.0000, 0.4200],\n",
      "        [1.0000, 0.5800],\n",
      "        [1.0000, 0.3300],\n",
      "        [1.0000, 0.1700]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataLoader\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#Validator data set\n",
    "if validator_data_dir != None :\n",
    "    validator_dataset=CustomDACDataset(data_dir=validator_data_dir, metadata_excel=validator_data_frames, Tt=Tt)\n",
    "    validator_dataloader= DataLoader(validator_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "#---------------------------------------------------------------\n",
    "# Test data dir\n",
    "for batch_idx, (inputs, targets, cvect) in enumerate(dataloader):\n",
    "    #pass\n",
    "    # Your training code here\n",
    "    # inputs: batch of input data of shape [batch_size, N, T-1]\n",
    "    # targets: corresponding batch of target data of shape [batch_size, N, T-1]\n",
    "    \n",
    "    if (batch_idx == 0) : \n",
    "        print(f\"Batch {batch_idx + 1}\")\n",
    "        print(f\"Inputs shape: {inputs.shape}\")\n",
    "        print(f\"Targets shape: {targets.shape}\")\n",
    "        print(f\"cvect shape: {cvect.shape}\")\n",
    "        print(f'cevect is {cvect}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb0eb6c",
   "metadata": {},
   "source": [
    "### <font color='blue'> Make the mask \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c9b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask.shape is torch.Size([430, 430])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [-inf, -inf, -inf,  ..., 0., -inf, -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., -inf],\n",
       "        [-inf, -inf, -inf,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = generate_mask(Tt, Ti).to(device)\n",
    "print(f'Mask.shape is {mask.shape}')\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11c4b392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model with embed_size=256, cond_size=2\n",
      "Setting up MultiEmbedding with vocab_size= 1024, embed_size= 256, num_codebooks= 4\n",
      "Total number of parameters: 4770816\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model, put it on the device\n",
    "#model = TransformerDecoder(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, num_tokens, vocab_size).to(device)\n",
    "print(f'Creating model with embed_size={embed_size}, cond_size={cond_size}')\n",
    "\n",
    "if start_epoch == 0 : \n",
    "    model = TransformerClass(embed_size, num_layers, num_heads, forward_expansion, dropout_rate, Tt, cond_classes, num_tokens, vocab_size, cond_size, use_adaLN=use_adaLN, verbose=verboselevel).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "else:\n",
    "    checkpoint_path = outdir+\"/\"+basefname+\"_chkpt_\"+str(start_epoch).zfill(4) +\".pth\"\n",
    "    print(f'in train, start_epoch = {start_epoch} and checkpoint_path = {checkpoint_path}')\n",
    "    assert os.path.exists(checkpoint_path), f\"{checkpoint_path} does not exist.\"\n",
    "    if start_epoch != 0 and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        print(f\"Loading and creating model from {checkpoint_path}\")       \n",
    "        # Restore model weights\n",
    "        model, optimizer, _, vocab_size, num_tokens, cond_size = load_model(checkpoint_path,  TransformerClass, device)\n",
    "        #best_metric = checkpoint['best_metric']  # If you're tracking performance      \n",
    "        print(f\"Resuming from epoch {start_epoch}\")\n",
    "   \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Total number of parameters: {num_params}')\n",
    "\n",
    "# Initialize SummaryWriter for tensorboard \n",
    "writer = SummaryWriter(outdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85526818-66e9-4bf0-9a94-0334ecd39d61",
   "metadata": {},
   "source": [
    "<div style=\"width: 100%; height: 20px; background-color: black;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf2f038",
   "metadata": {},
   "source": [
    "# <font color='blue'> Train !! \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccb439d-6d8f-4b1c-a4fd-6acac2a07db6",
   "metadata": {},
   "source": [
    "### loss is average CE across all output tokens\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{N} \\sum_{n=1}^{N} \\text{CE}(x_n, y_n)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19920362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 100  (with max 10000), loss: 0.33687323331832886\n",
      "train time for 100 epochs, was 5.13069486618042\n",
      "\n",
      "EPOCH 200  (with max 10000), loss: 0.049487438052892685\n",
      "train time for 200 epochs, was 10.14552927017212\n",
      "\n",
      "EPOCH 300  (with max 10000), loss: 0.02432779222726822\n",
      "train time for 300 epochs, was 15.20638656616211\n",
      "\n",
      "EPOCH 400  (with max 10000), loss: 0.014159697107970715\n",
      "train time for 400 epochs, was 20.2152841091156\n",
      "\n",
      "EPOCH 500  (with max 10000), loss: 0.012629740871489048\n",
      "train time for 500 epochs, was 25.284355878829956\n",
      "\n",
      "EPOCH 600  (with max 10000), loss: 0.008515321649610996\n",
      "train time for 600 epochs, was 30.32163977622986\n",
      "\n",
      "EPOCH 700  (with max 10000), loss: 0.007742410525679588\n",
      "train time for 700 epochs, was 35.331530809402466\n",
      "\n",
      "EPOCH 800  (with max 10000), loss: 0.006418380420655012\n",
      "train time for 800 epochs, was 40.340437173843384\n",
      "\n",
      "EPOCH 900  (with max 10000), loss: 0.004422883037477732\n",
      "train time for 900 epochs, was 45.34810256958008\n",
      "\n",
      "EPOCH 1000  (with max 10000), loss: 0.006515069864690304\n",
      "train time for 1000 epochs, was 50.35658645629883\n",
      "\n",
      "EPOCH 1000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_1000.pth\n",
      "\n",
      "EPOCH 1100  (with max 10000), loss: 0.0050029996782541275\n",
      "train time for 1100 epochs, was 55.429410219192505\n",
      "\n",
      "EPOCH 1200  (with max 10000), loss: 0.004935102537274361\n",
      "train time for 1200 epochs, was 60.43781089782715\n",
      "\n",
      "EPOCH 1300  (with max 10000), loss: 0.005552453920245171\n",
      "train time for 1300 epochs, was 65.40911340713501\n",
      "\n",
      "EPOCH 1400  (with max 10000), loss: 0.0035155052319169044\n",
      "train time for 1400 epochs, was 70.41902947425842\n",
      "\n",
      "EPOCH 1500  (with max 10000), loss: 0.0023168709594756365\n",
      "train time for 1500 epochs, was 75.38984441757202\n",
      "\n",
      "EPOCH 1600  (with max 10000), loss: 0.0044199931435287\n",
      "train time for 1600 epochs, was 80.36250853538513\n",
      "\n",
      "EPOCH 1700  (with max 10000), loss: 0.0024283567909151316\n",
      "train time for 1700 epochs, was 85.33450484275818\n",
      "\n",
      "EPOCH 1800  (with max 10000), loss: 0.001973393838852644\n",
      "train time for 1800 epochs, was 90.3063600063324\n",
      "\n",
      "EPOCH 1900  (with max 10000), loss: 0.002202184172347188\n",
      "train time for 1900 epochs, was 95.30462956428528\n",
      "\n",
      "EPOCH 2000  (with max 10000), loss: 0.004992549307644367\n",
      "train time for 2000 epochs, was 100.29591345787048\n",
      "\n",
      "EPOCH 2000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_2000.pth\n",
      "\n",
      "EPOCH 2100  (with max 10000), loss: 0.004042265936732292\n",
      "train time for 2100 epochs, was 105.40065503120422\n",
      "\n",
      "EPOCH 2200  (with max 10000), loss: 0.003195239696651697\n",
      "train time for 2200 epochs, was 110.44966268539429\n",
      "\n",
      "EPOCH 2300  (with max 10000), loss: 0.0017640164587646723\n",
      "train time for 2300 epochs, was 115.4587230682373\n",
      "\n",
      "EPOCH 2400  (with max 10000), loss: 0.0023648347705602646\n",
      "train time for 2400 epochs, was 120.50890231132507\n",
      "\n",
      "EPOCH 2500  (with max 10000), loss: 0.0032051559537649155\n",
      "train time for 2500 epochs, was 125.60132336616516\n",
      "\n",
      "EPOCH 2600  (with max 10000), loss: 0.003215288044884801\n",
      "train time for 2600 epochs, was 130.62269854545593\n",
      "\n",
      "EPOCH 2700  (with max 10000), loss: 0.002095483709126711\n",
      "train time for 2700 epochs, was 135.62849807739258\n",
      "\n",
      "EPOCH 2800  (with max 10000), loss: 0.003966846503317356\n",
      "train time for 2800 epochs, was 140.679913520813\n",
      "\n",
      "EPOCH 2900  (with max 10000), loss: 0.0019288054900243878\n",
      "train time for 2900 epochs, was 145.7113299369812\n",
      "\n",
      "EPOCH 3000  (with max 10000), loss: 0.0014284811913967133\n",
      "train time for 3000 epochs, was 150.74235439300537\n",
      "\n",
      "EPOCH 3000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_3000.pth\n",
      "\n",
      "EPOCH 3100  (with max 10000), loss: 0.0008825151599012315\n",
      "train time for 3100 epochs, was 155.83499431610107\n",
      "\n",
      "EPOCH 3200  (with max 10000), loss: 0.00297661405056715\n",
      "train time for 3200 epochs, was 160.82292342185974\n",
      "\n",
      "EPOCH 3300  (with max 10000), loss: 0.001376924803480506\n",
      "train time for 3300 epochs, was 165.8183991909027\n",
      "\n",
      "EPOCH 3400  (with max 10000), loss: 0.0030829093884676695\n",
      "train time for 3400 epochs, was 170.82388997077942\n",
      "\n",
      "EPOCH 3500  (with max 10000), loss: 0.001832766574807465\n",
      "train time for 3500 epochs, was 175.8021354675293\n",
      "\n",
      "EPOCH 3600  (with max 10000), loss: 0.0028817804995924234\n",
      "train time for 3600 epochs, was 180.8066635131836\n",
      "\n",
      "EPOCH 3700  (with max 10000), loss: 0.0023933574557304382\n",
      "train time for 3700 epochs, was 185.8021514415741\n",
      "\n",
      "EPOCH 3800  (with max 10000), loss: 0.0013872465351596475\n",
      "train time for 3800 epochs, was 190.81485271453857\n",
      "\n",
      "EPOCH 3900  (with max 10000), loss: 0.0023444457910954952\n",
      "train time for 3900 epochs, was 195.79012203216553\n",
      "\n",
      "EPOCH 4000  (with max 10000), loss: 0.0014596981927752495\n",
      "train time for 4000 epochs, was 200.7637963294983\n",
      "\n",
      "EPOCH 4000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_4000.pth\n",
      "\n",
      "EPOCH 4100  (with max 10000), loss: 0.0007168658776208758\n",
      "train time for 4100 epochs, was 205.83525466918945\n",
      "\n",
      "EPOCH 4200  (with max 10000), loss: 0.0019362312741577625\n",
      "train time for 4200 epochs, was 210.8082504272461\n",
      "\n",
      "EPOCH 4300  (with max 10000), loss: 0.0006285684066824615\n",
      "train time for 4300 epochs, was 215.78250861167908\n",
      "\n",
      "EPOCH 4400  (with max 10000), loss: 0.0017064096173271537\n",
      "train time for 4400 epochs, was 220.7551383972168\n",
      "\n",
      "EPOCH 4500  (with max 10000), loss: 0.0018918978748843074\n",
      "train time for 4500 epochs, was 225.7288830280304\n",
      "\n",
      "EPOCH 4600  (with max 10000), loss: 0.0027548072393983603\n",
      "train time for 4600 epochs, was 230.70223879814148\n",
      "\n",
      "EPOCH 4700  (with max 10000), loss: 0.0005822780658490956\n",
      "train time for 4700 epochs, was 235.67571711540222\n",
      "\n",
      "EPOCH 4800  (with max 10000), loss: 0.001395727158524096\n",
      "train time for 4800 epochs, was 240.64796781539917\n",
      "\n",
      "EPOCH 4900  (with max 10000), loss: 0.0008169874199666083\n",
      "train time for 4900 epochs, was 245.62552094459534\n",
      "\n",
      "EPOCH 5000  (with max 10000), loss: 0.0008420961676165462\n",
      "train time for 5000 epochs, was 250.64305567741394\n",
      "\n",
      "EPOCH 5000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_5000.pth\n",
      "\n",
      "EPOCH 5100  (with max 10000), loss: 0.0014410243602469563\n",
      "train time for 5100 epochs, was 255.71670603752136\n",
      "\n",
      "EPOCH 5200  (with max 10000), loss: 0.0028125953394919634\n",
      "train time for 5200 epochs, was 260.7248594760895\n",
      "\n",
      "EPOCH 5300  (with max 10000), loss: 0.0018397009698674083\n",
      "train time for 5300 epochs, was 265.73352885246277\n",
      "\n",
      "EPOCH 5400  (with max 10000), loss: 0.0019388789078220725\n",
      "train time for 5400 epochs, was 270.74642181396484\n",
      "\n",
      "EPOCH 5500  (with max 10000), loss: 0.0005864310660399497\n",
      "train time for 5500 epochs, was 275.7906563282013\n",
      "\n",
      "EPOCH 5600  (with max 10000), loss: 0.0004489200364332646\n",
      "train time for 5600 epochs, was 280.8001027107239\n",
      "\n",
      "EPOCH 5700  (with max 10000), loss: 0.0018512250389903784\n",
      "train time for 5700 epochs, was 285.8325664997101\n",
      "\n",
      "EPOCH 5800  (with max 10000), loss: 0.0005339901545085013\n",
      "train time for 5800 epochs, was 290.842102766037\n",
      "\n",
      "EPOCH 5900  (with max 10000), loss: 0.001609420869499445\n",
      "train time for 5900 epochs, was 295.8705406188965\n",
      "\n",
      "EPOCH 6000  (with max 10000), loss: 0.0011558544356375933\n",
      "train time for 6000 epochs, was 300.87841510772705\n",
      "\n",
      "EPOCH 6000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_6000.pth\n",
      "\n",
      "EPOCH 6100  (with max 10000), loss: 0.0007506223628297448\n",
      "train time for 6100 epochs, was 305.9965310096741\n",
      "\n",
      "EPOCH 6200  (with max 10000), loss: 0.0004402054182719439\n",
      "train time for 6200 epochs, was 311.0078146457672\n",
      "\n",
      "EPOCH 6300  (with max 10000), loss: 0.00034700491232797503\n",
      "train time for 6300 epochs, was 316.0342195034027\n",
      "\n",
      "EPOCH 6400  (with max 10000), loss: 0.0015640704659745097\n",
      "train time for 6400 epochs, was 321.08050298690796\n",
      "\n",
      "EPOCH 6500  (with max 10000), loss: 0.0015953767579048872\n",
      "train time for 6500 epochs, was 326.1154429912567\n",
      "\n",
      "EPOCH 6600  (with max 10000), loss: 0.0016203211853280663\n",
      "train time for 6600 epochs, was 331.1347200870514\n",
      "\n",
      "EPOCH 6700  (with max 10000), loss: 0.0015468000201508403\n",
      "train time for 6700 epochs, was 336.1371386051178\n",
      "\n",
      "EPOCH 6800  (with max 10000), loss: 0.000678514945320785\n",
      "train time for 6800 epochs, was 341.14555072784424\n",
      "\n",
      "EPOCH 6900  (with max 10000), loss: 0.001087778015062213\n",
      "train time for 6900 epochs, was 346.1917598247528\n",
      "\n",
      "EPOCH 7000  (with max 10000), loss: 0.0017249977681785822\n",
      "train time for 7000 epochs, was 351.24532103538513\n",
      "\n",
      "EPOCH 7000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_7000.pth\n",
      "\n",
      "EPOCH 7100  (with max 10000), loss: 0.00075875612674281\n",
      "train time for 7100 epochs, was 356.32845640182495\n",
      "\n",
      "EPOCH 7200  (with max 10000), loss: 0.001310192747041583\n",
      "train time for 7200 epochs, was 361.3711748123169\n",
      "\n",
      "EPOCH 7300  (with max 10000), loss: 0.0006387822213582695\n",
      "train time for 7300 epochs, was 366.44251585006714\n",
      "\n",
      "EPOCH 7400  (with max 10000), loss: 0.0008708040113560855\n",
      "train time for 7400 epochs, was 371.4333689212799\n",
      "\n",
      "EPOCH 7500  (with max 10000), loss: 0.0014074388891458511\n",
      "train time for 7500 epochs, was 376.4424469470978\n",
      "\n",
      "EPOCH 7600  (with max 10000), loss: 0.0006681812228634953\n",
      "train time for 7600 epochs, was 381.45074915885925\n",
      "\n",
      "EPOCH 7700  (with max 10000), loss: 0.0009300410747528076\n",
      "train time for 7700 epochs, was 386.4596185684204\n",
      "\n",
      "EPOCH 7800  (with max 10000), loss: 0.00035141088301315904\n",
      "train time for 7800 epochs, was 391.4561505317688\n",
      "\n",
      "EPOCH 7900  (with max 10000), loss: 0.00044852818245999515\n",
      "train time for 7900 epochs, was 396.47703790664673\n",
      "\n",
      "EPOCH 8000  (with max 10000), loss: 0.001583956414833665\n",
      "train time for 8000 epochs, was 401.4682106971741\n",
      "\n",
      "EPOCH 8000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_8000.pth\n",
      "\n",
      "EPOCH 8100  (with max 10000), loss: 0.0003538716700859368\n",
      "train time for 8100 epochs, was 406.5395607948303\n",
      "\n",
      "EPOCH 8200  (with max 10000), loss: 0.0003803469007834792\n",
      "train time for 8200 epochs, was 411.57831621170044\n",
      "\n",
      "EPOCH 8300  (with max 10000), loss: 0.0010619000531733036\n",
      "train time for 8300 epochs, was 416.5869014263153\n",
      "\n",
      "EPOCH 8400  (with max 10000), loss: 0.0007833492127247155\n",
      "train time for 8400 epochs, was 421.6501467227936\n",
      "\n",
      "EPOCH 8500  (with max 10000), loss: 0.0007256602984853089\n",
      "train time for 8500 epochs, was 426.7008361816406\n",
      "\n",
      "EPOCH 8600  (with max 10000), loss: 0.0005292565329000354\n",
      "train time for 8600 epochs, was 431.7482898235321\n",
      "\n",
      "EPOCH 8700  (with max 10000), loss: 0.000563570240046829\n",
      "train time for 8700 epochs, was 436.7564911842346\n",
      "\n",
      "EPOCH 8800  (with max 10000), loss: 0.0012424529995769262\n",
      "train time for 8800 epochs, was 441.7634222507477\n",
      "\n",
      "EPOCH 8900  (with max 10000), loss: 0.0005290595581755042\n",
      "train time for 8900 epochs, was 446.76811361312866\n",
      "\n",
      "EPOCH 9000  (with max 10000), loss: 0.00030765210976824164\n",
      "train time for 9000 epochs, was 451.8236711025238\n",
      "\n",
      "EPOCH 9000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_9000.pth\n",
      "\n",
      "EPOCH 9100  (with max 10000), loss: 0.00039294868474826217\n",
      "train time for 9100 epochs, was 456.89378690719604\n",
      "\n",
      "EPOCH 9200  (with max 10000), loss: 0.0008115092641673982\n",
      "train time for 9200 epochs, was 461.9473247528076\n",
      "\n",
      "EPOCH 9300  (with max 10000), loss: 0.00032806419767439365\n",
      "train time for 9300 epochs, was 466.99315762519836\n",
      "\n",
      "EPOCH 9400  (with max 10000), loss: 0.00043353420915082097\n",
      "train time for 9400 epochs, was 472.0017352104187\n",
      "\n",
      "EPOCH 9500  (with max 10000), loss: 0.0010819967137649655\n",
      "train time for 9500 epochs, was 477.01011538505554\n",
      "\n",
      "EPOCH 9600  (with max 10000), loss: 0.0006608153926208615\n",
      "train time for 9600 epochs, was 482.0181224346161\n",
      "\n",
      "EPOCH 9700  (with max 10000), loss: 0.00022027025988791138\n",
      "train time for 9700 epochs, was 487.0636100769043\n",
      "\n",
      "EPOCH 9800  (with max 10000), loss: 0.00023565941955894232\n",
      "train time for 9800 epochs, was 492.0901458263397\n",
      "\n",
      "EPOCH 9900  (with max 10000), loss: 0.00017587233742233366\n",
      "train time for 9900 epochs, was 497.11661171913147\n",
      "\n",
      "EPOCH 10000  (with max 10000), loss: 0.00032267035567201674\n",
      "train time for 10000 epochs, was 502.161922454834\n",
      "\n",
      "EPOCH 10000 save model to : runs/2025.03.07b_lala_test_256_float_FiLM_CondEmbed/out.e256.l4.h8_chkpt_10000.pth\n",
      "\n",
      "train time for 10000 epochs, was 502.2283580303192\n",
      "loss  =  0.00032267035567201674\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train(model, optimizer, dataloader, num_epochs, device, outdir, basefname, start_epoch=0, checkpoint_path=None):\n",
    "    t0 = time.time()\n",
    "    max_epoch = start_epoch + num_epochs\n",
    "    for epoch in range(start_epoch, max_epoch):\n",
    "        torch.cuda.empty_cache()\n",
    "        model.train()\n",
    "        for batch_idx, (input_data, target_data, cond_data) in enumerate(dataloader):\n",
    "            if verboselevel > 5 :\n",
    "                print(f' ---- submitting batch with input_data={input_data.shape}, target_data={target_data.shape}, cond_data={cond_data.shape}')\n",
    "            #print(f\"b{batch_idx} \", end='')\n",
    "            optimizer.zero_grad()\n",
    "    \n",
    "            # Move inputs and targets to the device\n",
    "            input_data, target_data, cond_data = input_data.to(device), target_data.to(device), cond_data.to(device)\n",
    "            \n",
    "            if cond_size==0 :  #Ignore conditioning data\n",
    "                cond_expanded=None\n",
    "            else : \n",
    "                # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "\n",
    "            if verboselevel > 9 :\n",
    "                print(f'    after loading a batch,  input_data.shape is {input_data.shape}, and cond_data.shape is {cond_data.shape}')\n",
    "                print(f'    after loading a batch,  cond_expanded.shape is {cond_expanded.shape}')\n",
    "                #print(f'    after loading a batch,  mask.shape is {mask.shape}')\n",
    "                #print(f' model={model}')\n",
    "            \n",
    "            # torch.Size([batch_size, seq_len, num_tokens, vocab_size])\n",
    "            output = model(input_data, cond_expanded, mask)\n",
    "        \n",
    "            if verboselevel > 5 :\n",
    "                print(f' TTTTTTTT after training, output shape ={output.shape}, torch.Size([batch_size, seq_len, num_tokens, vocab_size])')\n",
    "                print(f' TTTTTTTT Passing to CRITERION with , output.reshape(-1, vocab_size) = {output.reshape(-1, vocab_size).shape} and target_data.reshape(-1) = {target_data.reshape(-1).shape}' )\n",
    "    \n",
    "            ##  this works, but is too verbose >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens, vocab_size)\n",
    "            ##      output = output.reshape(batch_size, sequence_length * num_tokens, vocab_size)\n",
    "            ##      # Original shape: (batch_size, seq_len, num_tokens)\n",
    "            ##      targets = targets.reshape(batch_size, sequence_length * num_tokens)\n",
    "            ##      loss = criterion(output.permute(0, 2, 1), targets) \n",
    "            \n",
    "            ##  more succinct <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            #   Computes the CE for each token separately, and then averages them to get the loss.\n",
    "            #loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1)) # collapses all target_data dimensions into a single dimension\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target_data.reshape(-1).long())\n",
    "            ## <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % ErrorLogRate == 0:\n",
    "            print(f'EPOCH {epoch+1}  (with max {max_epoch}), ', end='')\n",
    "            print(f'loss: {loss}')\n",
    "            # Log the loss to TensorBoard\n",
    "            writer.add_scalar('Loss/train', loss, epoch)\n",
    "            \n",
    "            if validator_data_dir != None :\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_loss = 0\n",
    "                    for val_inputs, val_targets, cond_data in validator_dataloader:\n",
    "                        val_inputs, val_targets, cond_data = val_inputs.to(device), val_targets.to(device), cond_data.to(device)\n",
    "                        \n",
    "                        if cond_size==0 :  #Ignore conditioning data\n",
    "                            cond_expanded=None\n",
    "                        else: \n",
    "                            # for dataset exammples, expand the conditioning info across all time steps before passing to models\n",
    "                            cond_expanded = cond_data.unsqueeze(1).expand(-1, input_data.size(1), -1)\n",
    "    \n",
    "                        \n",
    "                        val_outputs = model(val_inputs,cond_expanded, mask)\n",
    "                        \n",
    "                        val_loss += criterion(val_outputs.reshape(-1, vocab_size), val_targets.reshape(-1).long()) # collapses all target_data dimensions into a single dimension\n",
    "                        #val_loss += criterion(val_outputs, val_targets).item()\n",
    "    \n",
    "                print(f'Validation Loss: {val_loss / len(validator_dataloader)}')\n",
    "                writer.add_scalar('Loss/validation', val_loss / len(validator_dataloader), epoch)\n",
    "    \n",
    "            t1 = time.time()\n",
    "            train_time = t1-t0\n",
    "            print(f'train time for {epoch-start_epoch+1} epochs, was {train_time}' )\n",
    "            print(f'')\n",
    "                \n",
    "        if (epoch+1) % checkpoint_interval == 0:\n",
    "            lastbasename = outdir+\"/\"+basefname+\"_chkpt_\"+str(epoch+1).zfill(4)\n",
    "            print(f'EPOCH {epoch+1} save model to : {lastbasename}.pth')\n",
    "            print(f'')\n",
    "            save_model(model, optimizer, Ti,  lastbasename +\".pth\")\n",
    "        \n",
    "    \n",
    "    t1 = time.time()\n",
    "    train_time = t1-t0\n",
    "    print(f'train time for {num_epochs} epochs, was {train_time}' )\n",
    "    print(f'loss  =  {loss}' )\n",
    "    \n",
    "## -----------------------------------------------------------------------------------\n",
    "## OK, let's do it!\n",
    "train(model, optimizer, dataloader, num_epochs, device, outdir, basefname, start_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16eb43b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
       "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#just check that inference attention mask will look right\n",
    "#Actually, the inference mask can be None since we are using a context window only as long as the maximum look-back in the training mask\n",
    "# thats why taking the mask with :TI is upper-triangular. Longer dims would show a banded mask again.\n",
    "foo=mask[:Ti, :Ti]\n",
    "foo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871a1f25-78a6-479b-8771-2910cf639d67",
   "metadata": {},
   "source": [
    "### <font color='blue'> Use Inference.Decode.ipynb to see and hear your generated audio   \n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68274809-4986-4d54-a442-ba7ac8b48ea0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
